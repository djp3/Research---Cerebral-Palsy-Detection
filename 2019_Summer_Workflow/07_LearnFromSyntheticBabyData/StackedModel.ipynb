{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A CNN model builder for analyzing inputs of both Optical Flow and Depth Flow by creating a 'stacked' image,\n",
    "which is a 4 channel image that is created by stacking each optical flow on top of a depth flow image. Model has not been\n",
    "run yet, and so changes may have to be made to the structure of the model. Nonetheless, this basic model is ready to be run\n",
    "and will hopefully learn something.\n",
    "'''\n",
    "\n",
    "#Import from the Keras library\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D,  MaxPooling2D\n",
    "from keras import optimizers \n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from secret import credentials\n",
    "\n",
    "#This allows for Keras models to be saved. \n",
    "import h5py\n",
    "#Other import statements \n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import cv2\n",
    "import pymysql\n",
    "import os\n",
    "\n",
    "from ModelTesting import tests\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#limit the data to a subset for testing\n",
    "#Make empty string to have no limit\n",
    "#limit = \"ORDER BY RAND() LIMIT 1000\"\n",
    "limit = ''\n",
    "epochs = 2\n",
    "batch_size = 1\n",
    "\n",
    "#directory where data from database is stored\n",
    "cache_path = 'RecordingCache'\n",
    "\n",
    "#File name for the statistics to be save in. Must include .txt at the end\n",
    "statistics_output_file = 'statistics.output.txt'\n",
    "\n",
    "#Must have the h5py package installed or the model will not save. This should be the path of the location you would like\n",
    "#To save the model\n",
    "model_file_name = 'model.output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect(): \n",
    "    db_host = credentials['db_host'];\n",
    "    db_port = credentials['db_port'];\n",
    "    db_name = credentials['db_name'];\n",
    "    db_username = credentials['db_username']\n",
    "    db_password = credentials['db_password']\n",
    "    \n",
    "    conn = pymysql.connect(db_host, user=db_username, port=db_port, passwd=db_password, db=db_name)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports either dflow or oflow from database\n",
    "def import_data(cache_path, img_type, conn=connect()):     \n",
    "    \n",
    "    \n",
    "    #Create the cache directory if it doesn't exist\n",
    "    if not os.path.exists(cache_path):\n",
    "        os.mkdir(cache_path)\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        recording_ids = {}\n",
    "        xy = {}    \n",
    "        \n",
    "        image_query = \"SELECT r.id, r.recording_id, r.isCSGM FROM nicu.Video_Raw AS r JOIN nicu.Video_Generated AS g ON r.id=g.raw_id  WHERE (r.recording_id>1) AND (g.RGB_Optical_Flow IS NOT NULL) AND (g.D_Depth_Flow IS NOT NULL) \" +limit\n",
    "        try:\n",
    "            cursor.execute(image_query)\n",
    "            for row in cursor.fetchall():\n",
    "                raw_id = row[0]\n",
    "                rec_id = row[1]\n",
    "                csgm = row[2]\n",
    "                if rec_id in recording_ids:\n",
    "                    recording_ids.get(rec_id).append(raw_id)\n",
    "                else:\n",
    "                    recording_ids.update({rec_id:[raw_id]})\n",
    "                xy.update({raw_id:[csgm]})\n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving ID's\", e)\n",
    "            conn.rollback()\n",
    "            raise e\n",
    "            \n",
    "        print(\"Collecting images for processing (o = source image in cache, ⇣ = source image fetched from db, x = source image not in db)\")\n",
    "        for rec_id in recording_ids:\n",
    "            print(\"\")\n",
    "            print(\"Analyzing recording_id:\",rec_id,\": \",end=\"\")\n",
    "            raw_id_list = recording_ids.get(rec_id)\n",
    "            for raw_id in raw_id_list:\n",
    "                current_input=0\n",
    "                if (img_type=='RGB_Optical_Flow'):\n",
    "                    current_input = cache_path+'/'+str(raw_id)+\".oflow.png\"\n",
    "                else:\n",
    "                    current_input = cache_path+'/'+str(raw_id)+\".dflow.png\"\n",
    "                if not os.path.exists(current_input):\n",
    "                    cursor2 = conn.cursor()\n",
    "                    try:\n",
    "                        image_query = \"SELECT \"+img_type+\" from Video_Generated WHERE (raw_id=%s)\"\n",
    "                        cursor2.execute(image_query, (str(raw_id)))\n",
    "                        for row in cursor2.fetchall():\n",
    "                            db_img = row[0]\n",
    "                            if db_img is not None:\n",
    "                                img=cv2.imdecode(np.asarray(bytearray(db_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                                cv2.imwrite(current_input,img)\n",
    "                                print(\"⇣\",end=\"\",flush=True)\n",
    "                            else:\n",
    "                                print(\"x\",end=\"\",flush=True)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error retrieving Optical Flow frame\",e)\n",
    "                        raise e\n",
    "                    finally:\n",
    "                        cursor2.close()     \n",
    "                else:\n",
    "                    print(\"o\",end=\"\",flush=True)\n",
    "\n",
    "                #Resizing the image\n",
    "                if img_type=='RGB_Optical_Flow':\n",
    "                    img = cv2.imread(current_input)\n",
    "                else:\n",
    "                    img=cv2.imread(current_input, cv2.IMREAD_ANYDEPTH)\n",
    "                scale_percent = 30\n",
    "                width = int(img.shape[1] * scale_percent / 100)\n",
    "                height = int(img.shape[0] * scale_percent / 100)\n",
    "                cv2.resize(img,(width,height), interpolation=cv2.INTER_CUBIC)                \n",
    "                xy.get(raw_id).insert(0,img)\n",
    "        print(\"\")\n",
    "        return recording_ids, xy\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates an array for input into model\n",
    "def create_array(raw_ids, xy):    \n",
    "    image_list=[]\n",
    "    csgm_list=[]\n",
    "    \n",
    "    random.shuffle(raw_ids)\n",
    "    \n",
    "    no_csgm=0\n",
    "    csgm=0\n",
    "    for i in raw_ids:\n",
    "        image_list.append(xy.get(i)[0])\n",
    "        csgm_list.append(xy.get(i)[1])\n",
    "        if (xy.get(i)[1]==0):\n",
    "            no_csgm += 1\n",
    "        else:\n",
    "            csgm += 1\n",
    "    \n",
    "    #if there are an uneven number of CSGM and non-CSGM images, duplicate frames until there is an even number \n",
    "    index = 0\n",
    "    difference = csgm - no_csgm\n",
    "    while (difference != 0):\n",
    "        if (csgm_list[index]==0 and difference > 0) or (csgm_list[index]==1 and difference < 0):\n",
    "            i = random.randint(0,len(csgm_list))\n",
    "            csgm_list.insert(i, csgm_list[index])\n",
    "            image_list.insert(i, image_list[index])\n",
    "            difference = (abs(difference)-1)*int(difference/difference)\n",
    "        if (index < len(csgm_list)-1):\n",
    "            index += 1\n",
    "        else:\n",
    "            index = 0\n",
    "    x = np.array(image_list)\n",
    "    y = np.array(csgm_list)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a CNN\n",
    "def create_cnn(x_train, filter_info={0:[32,3]}, dropout={0:0.25}, pooling={0:2}, activation='relu', loss='mean_squared_error', final_activation='sigmoid'):    \n",
    "    \n",
    "    model = models.Sequential() \n",
    "    str_model = \"Overview of Model Architecture: /n\"\n",
    "    \n",
    "    filter_size = 0\n",
    "    \n",
    "    #loops through inputs to create layers\n",
    "    for i in filter_info: \n",
    "        filter_size = filter_info.get(i)[1]\n",
    "        num_filters = filter_info.get(i)[0]\n",
    "        \n",
    "        if i == 0: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation = 'relu', input_shape=x_train.shape[1:]))\n",
    "        else: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation= 'relu'))\n",
    "        \n",
    "        str_model += (\"2D Convolution Layer with %d filters the size of (%d,%d) and %s activation \\n\" %(num_filters, filter_size, filter_size, activation))\n",
    "                \n",
    "        if i in pooling:           \n",
    "            pool_filter_size = pooling.get(i)\n",
    "            model.add(MaxPooling2D(pool_size=(pool_filter_size, pool_filter_size)))\n",
    "            str_model += ('2D Pooling Max Pooling Layer with filter size (%d,%d)\\n' %(pool_filter_size,pool_filter_size))\n",
    "            \n",
    "            \n",
    "        if i in dropout: \n",
    "            drop_rate = dropout.get(i)\n",
    "            model.add(Dropout(drop_rate))\n",
    "            str_model += ('Droput Layer with with a rate of %f \\n' %(drop_rate))\n",
    "\n",
    "\n",
    "    \n",
    "    #These will be added to the end of every model no matter what\n",
    "    model.add(Flatten())\n",
    "    str_model += ('Flatten\\n')\n",
    "    model.add(Dense(256,activation=activation))\n",
    "    str_model += ('Dense layer with %s activation\\n' %(activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    str_model += ('Droput Layer with with a rate of 0.5 \\n')\n",
    "    \n",
    "    #Sigmoid activiation is employed in the final step because the output is binary. \n",
    "    model.add(Dense(1, activation=final_activation)) \n",
    "    str_model += ('Dense layer with %s activation\\n' %(final_activation))\n",
    "    \n",
    "    model.compile(loss=loss, \n",
    "              optimizer=optimizers.SGD(lr=1e-4),\n",
    "              metrics=['acc']) \n",
    "    str_model += ('Loss: %s' %(loss))\n",
    "                      \n",
    "    print(str_model)\n",
    "    \n",
    "    return model, str_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(exp_values, predicted_values):\n",
    "    \"\"\"\n",
    "    This creates a confusion matrix with the predicted accuracy of the model.\n",
    "    \n",
    "    exp_values must be in the format of a list and predicted values is expected to come in the format of the ouput \n",
    "    of Keras's model.predict()\n",
    "    \n",
    "    The ouput is a pandas dataframe that displays a confusion matrix indicitive of the accuracy of the model along \n",
    "        with a number score which is the accuracy of the model.\n",
    "    \"\"\"\n",
    "    predicted_values = convert_predictions(predicted_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Creates a DataFrame of zeros\n",
    "    matrix = pd.DataFrame(np.zeros((2,2)) , ['P0','P1'], ['E0','E1'])\n",
    "   \n",
    "    #Caculates whether the score was right or wrong and updates the confusion matrix \n",
    "    for i in range(len(exp_values)):\n",
    "        if exp_values[i] == predicted_values[i]:\n",
    "            matrix.iloc[[predicted_values[i]],[predicted_values[i]]] += 1\n",
    "        else:\n",
    "            matrix.iloc[[predicted_values[i]],[exp_values[i]]] += 1\n",
    "   \n",
    "   \n",
    "    diagonal_sum = 0\n",
    "    for i in range(2):\n",
    "        diagonal_sum += matrix.iloc[i][i]\n",
    "    \n",
    "    score = diagonal_sum/len(exp_values)\n",
    "    \n",
    "  \n",
    "    return  matrix, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_predictions(predictions): \n",
    "    \"\"\"\n",
    "    Converts predictions outputted by a keras model into a list with 1 represented the predicted output and zero \n",
    "    in other classes. \n",
    "    \"\"\"\n",
    "    l =[]\n",
    "    for p in predictions: \n",
    "        if p >= 0.5:\n",
    "            l.append(1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runTest(pooling, dropout, filter_info, loss, activation, final_activation, file_name='model.txt', model_name='model', save_model=False, epochs=5, batch_size=32, stacked=False):\n",
    "    \n",
    "    conn = connect()\n",
    "    try:\n",
    "        #imports appropriate images\n",
    "        recording_ids_dict, xy = import_data(cache_path,'RGB_Optical_Flow',conn)\n",
    "        #if stacked parameter is true, combine each dflow and oflow into a single 4 channel image\n",
    "        if (stacked):\n",
    "            rec_idDF, xyDF = import_data(cache_path,'D_Depth_Flow',conn)  \n",
    "            for i in xy:\n",
    "                xy[i][0]=np.dstack((xy[i][0], xyDF[i][0]))\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    matrices = {}\n",
    "    scores = {}\n",
    "    model_scores = {}\n",
    "    str_model =''\n",
    "\n",
    "    #iterate through the recordings, using each one as a test and training on the rest\n",
    "    for i in recording_ids_dict:\n",
    "        print('Testing on ' + str(i))\n",
    "        train_ids= []\n",
    "        test_ids = []\n",
    "        \n",
    "        for j in recording_ids_dict:\n",
    "            if j == i:\n",
    "                test_ids = recording_ids_dict[j]\n",
    "            else: \n",
    "                train_ids.extend(list(recording_ids_dict[j]))\n",
    "        \n",
    "        #create corresponding arrays for input\n",
    "        x_train, y_train = create_array(train_ids, xy)\n",
    "        x_test, y_test = create_array(test_ids, xy)\n",
    "        \n",
    "        #Scaling the values to a value between 0 and 1\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        \n",
    "        #create the model\n",
    "        model,str_model = create_cnn(x_train,\n",
    "                                     filter_info=filter_info,\n",
    "                                     dropout=dropout,\n",
    "                                     pooling=pooling,\n",
    "                                     loss=loss,\n",
    "                                     final_activation=final_activation,\n",
    "                                     activation=activation)\n",
    "        #optionally prints the structure of the model into the console and saves a png of the structure\n",
    "        #print(model.summary())\n",
    "        #plot_model(model, to_file='model_plot'+str(i)+'.png', show_shapes=True, show_layer_names=True)\n",
    "        \n",
    "        \n",
    "        #creates an early stopping callback that will end the training process \n",
    "        #if the loss has not changed after 100 epochs, and prints the last epoch\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "        #creates a callback that will save the model with the best accuracy, and prints that epoch number \n",
    "        mc = ModelCheckpoint(model_name+'.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "        \n",
    "        #Fit the model\n",
    "        history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs = epochs, callbacks=[es, mc] )\n",
    "        \n",
    "        #basic fitting choice if no callbacks are needed\n",
    "        #history = model.fit(x_train, y_train, epochs = epochs)\n",
    "        \n",
    "        #Create predictions and evaluate to find loss and accuaracy\n",
    "        \n",
    "        model_score = model.evaluate(x_test, y_test)\n",
    "        \n",
    "        '''\n",
    "        #optional line graph of the training and testing accuracy\n",
    "        pyplot.plot(history.history['loss'], label='train')\n",
    "        pyplot.plot(history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "        pyplot.savefig(model_name+'.png')\n",
    "        '''\n",
    "        \n",
    "        predict = model.predict(x_test)\n",
    "        print('Model was ' + str(model_score[1]*100) + '% accurate and exhibited an average loss of ' + str(model_score[0]) + '.')\n",
    "        \n",
    "        matrix,score = confusion_matrix(y_test, predict)\n",
    "        \n",
    "        matrices.update({i : matrix})\n",
    "        print(str(matrix) + '\\n')\n",
    "        scores.update({i: score})\n",
    "        print(str(score) + '\\n')\n",
    "        model_scores.update({i:model_score})\n",
    "   \n",
    "    with open(file_name, 'w') as f:\n",
    "        for key in matrices:\n",
    "            f.write(\"Baby %s\\n\" % key)\n",
    "            f.write(\"%s\\n\" % str_model)\n",
    "            f.write(\"%s\\n\" % matrices[key])\n",
    "            f.write(\"%s\\n\" % scores[key])\n",
    "            f.write(\"%s\\n\" % model_scores[key])\n",
    "        \n",
    "            \n",
    "    if save_model : \n",
    "        model.save(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a standard filter template\n",
    "filter_info={0:[32,3],1:[64,3],2:[128,3]}\n",
    "dropout={0:0.25,1:0.5,2:0.5}\n",
    "pooling={0:2,1:2,2:2}\n",
    "\n",
    "\n",
    "runTest(file_name=statistics_output_file, \n",
    "        filter_info=filter_info, \n",
    "        dropout=dropout, \n",
    "        pooling=pooling, \n",
    "        loss='mean_squared_error', \n",
    "        activation='relu',\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        final_activation='sigmoid',\n",
    "        model_name=model_file_name,\n",
    "        save_model=False,\n",
    "        stacked=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
