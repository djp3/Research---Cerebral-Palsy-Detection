{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import from the Keras library\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten, Input \n",
    "from keras.layers import Conv2D,  MaxPooling2D, TimeDistributed, LSTM, Conv3D, MaxPooling3D\n",
    "from keras.models import Model\n",
    "from keras import optimizers \n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "#from secret import credentials\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "#This allows for Keras models to be saved. \n",
    "import h5py\n",
    "#Other import statements \n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import cv2\n",
    "import pymysql\n",
    "import os\n",
    "\n",
    "#pick which recordings you are working with\n",
    "where_clause = \"WHERE (r.recording_id > 1)\"\n",
    "#limit the data to a subset for testing\n",
    "#Make empty string to have no limit\n",
    "#limit = \"ORDER BY RAND(1) LIMIT 3000\"\n",
    "limit_clause = \"LIMIT 1500\"\n",
    "\n",
    "#What size should we reduce images to before learning in percent\n",
    "#image_scale_percent = 1\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "max_accuracy = 0.90 #Quit if accuracy reaches this at any epoch\n",
    "\n",
    "#directory where data from database is stored\n",
    "cache_path = 'cache'\n",
    "\n",
    "#File name for the statistics to be save in. Must include .txt at the end\n",
    "statistics_output_file = 'statistics.sequential.output.txt'\n",
    "\n",
    "#Must have the h5py package installed or the model will not save. This should be the path of the location you would like\n",
    "#To save the model\n",
    "model_file_name = 'model.output'\n",
    "\n",
    "#The amount of frames to include in every sequence\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Secrets shouldn't be in the repository\n",
    "from secrets import credentials\n",
    "\n",
    "def connect(): \n",
    "    def connect(): \n",
    "    db_host = credentials['db_host'];\n",
    "    db_port = credentials['db_port'];\n",
    "    db_name = credentials['db_name'];\n",
    "    db_username = credentials['db_username']\n",
    "    db_password = credentials['db_password']\n",
    "    \n",
    "    conn = pymysql.connect(db_host, user=db_username, port=db_port, passwd=db_password, db=db_name)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_class_data(conn): \n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        #a dictionary that maps classes to a dictionary that maps recording_ids to lists of raw_ids\n",
    "        ''' 2 classes , 2 recording ids, 15 frames\n",
    "            {\n",
    "                0:{\n",
    "                    2:{[1,2,3,4],[2,3,4,5]},\n",
    "                    3:{[6,7,8,9],[7,8,9,10]}\n",
    "                },\n",
    "                1:{\n",
    "                    2:{[3,4,5,6],[4,5,6,7,8]},\n",
    "                    3:{[8,9,10,11,12],[7,8,9,10]}\n",
    "                }\n",
    "            }\n",
    "        '''\n",
    "                    \n",
    "        class_data={}\n",
    "        rec_data={}\n",
    "        \n",
    "        recording_query = \"SELECT r.id, r.recording_id, r.isCSGM FROM nicu.Video_Raw AS r JOIN nicu.Video_Generated AS g ON r.id=g.raw_id \"+where_clause+\" \"+limit_clause\n",
    "        try:\n",
    "            cursor.execute(recording_query)\n",
    "            for row in cursor.fetchall():\n",
    "                raw_id = row[0]\n",
    "                rec_id = row[1]\n",
    "                csgm = row[2]\n",
    "                \n",
    "                if rec_id not in rec_data:\n",
    "                    rec_data[rec_id] = list()\n",
    "                    \n",
    "                rec_data[rec_id].append([raw_id, csgm])               \n",
    "                    \n",
    "                        \n",
    "            for rec_id in rec_data:\n",
    "                raw_ids = rec_data[rec_id]\n",
    "                for i in range(len(raw_ids)):\n",
    "                    #This takes sequence at position n and will add the other raw_ids before n and creates a list the length\n",
    "                    #of sequence_length so that if sequence_length = 3 the sequence will be [n-2,n-1,n]\n",
    "                    if i >= sequence_length:\n",
    "                        frame = raw_ids[i]\n",
    "                        raw_id = frame[0]\n",
    "                        csgm = frame[1]\n",
    "\n",
    "                        j=0\n",
    "                        sequence = list()\n",
    "                        sequence.append(raw_id)\n",
    "                        while j < (sequence_length-1):\n",
    "                            if frame[0] - raw_ids[i-j][0] == j:\n",
    "                                sequence.insert(0,raw_ids[i-j][0])\n",
    "                            else: \n",
    "                                break\n",
    "                            j+=1\n",
    "                        \n",
    "                        if csgm not in class_data:\n",
    "                            class_data[csgm] = dict()\n",
    "                        \n",
    "                        \n",
    "                        single_class_data = class_data[csgm]\n",
    "\n",
    "                        if rec_id not in single_class_data:\n",
    "                            single_class_data[rec_id] = list()    \n",
    "                        \n",
    "                        single_class_data[rec_id].append(sequence)\n",
    "                        \n",
    "                    \n",
    "                        \n",
    "                                                      \n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving ID's\", e)\n",
    "            conn.rollback()\n",
    "            raise e\n",
    "            \n",
    "        return class_data\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_array(conn, class_data, batch_sequences):     \n",
    "    \n",
    "    #Find the true class of the elements in the batch\n",
    "    csgms = list() #The raw_id mapped to the class\n",
    "    ordered_sequences = list()\n",
    "    all_classes = class_data.keys()\n",
    "    for a_class in all_classes:\n",
    "        for recording_id, sequences in class_data[a_class].items():\n",
    "            for sequence in sequences:\n",
    "                if sequence in batch_sequences:\n",
    "                    csgms.append(a_class)\n",
    "                    ordered_sequences.append(sequence)\n",
    "\n",
    "    \n",
    "    #Retrieve the images\n",
    "    \n",
    "    if not os.path.exists(cache_path):\n",
    "        os.mkdir(cache_path)\n",
    "    \n",
    "    sequence_list=list()\n",
    "    csgm_list=list()\n",
    "    rgb_sequence_list=list()\n",
    "    d_sequence_list=list()\n",
    "    \n",
    "    for i in range(len(csgms)):\n",
    "        sequence = ordered_sequences[i]\n",
    "        the_class = csgms[i]\n",
    "        image_list=[]\n",
    "        rgb_image_list=list()\n",
    "        d_image_list=list()\n",
    "        for frame in sequence: \n",
    "            #Check the cache for the images in question\n",
    "            current_rgb_input = cache_path+'/'+str(frame)+\".oflow.png\"\n",
    "            current_d_input = cache_path+'/'+str(frame)+\".dflow.png\"\n",
    "            if not os.path.exists(current_d_input) or not os.path.exists(current_rgb_input):\n",
    "                cursor = conn.cursor()\n",
    "                try:\n",
    "                    image_query = \"SELECT RGB_Optical_Flow, D_Depth_Flow from Video_Generated WHERE (raw_id=%s)\"\n",
    "                    cursor.execute(image_query, (str(frame)))\n",
    "                    for row in cursor.fetchall():\n",
    "                        db_rgb_img = row[0]\n",
    "                        db_d_img = row[1]\n",
    "                        if (db_rgb_img is not None):\n",
    "                            rgb_img=cv2.imdecode(np.asarray(bytearray(db_rgb_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                            cv2.imwrite(current_rgb_input,rgb_img)\n",
    "                            print(\"⇣\",end=\"\",flush=True)\n",
    "                        else:\n",
    "                            print(\"x\",end=\"\",flush=True)\n",
    "                        if (db_d_img is not None):    \n",
    "                            d_img=cv2.imdecode(np.asarray(bytearray(db_d_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                            cv2.imwrite(current_d_input,d_img)\n",
    "                            print(\"⇣\",end=\"\",flush=True)\n",
    "                        else:\n",
    "                            print(\"x\",end=\"\",flush=True)\n",
    "                except Exception as e:\n",
    "                    print(\"Error retrieving frame\",e)\n",
    "                    raise e\n",
    "                finally:\n",
    "                    cursor.close()     \n",
    "            else:\n",
    "                print(\"o\",end=\"\",flush=True)\n",
    "\n",
    "            rgb_img = cv2.imread(current_rgb_input)\n",
    "            d_img  = cv2.imread(current_d_input)\n",
    "            if rgb_img is not None and d_img is not None: \n",
    "                #Resizing the image\n",
    "                #width = int(img.shape[1] * image_scale_percent / 100)\n",
    "                #height = int(img.shape[0] * image_scale_percent / 100)\n",
    "                #cv2.resize(img,(width,height), interpolation=cv2.INTER_CUBIC)\n",
    "                rgb_image_list.append(rgb_img)\n",
    "                d_image_list.append(d_img)\n",
    "        \n",
    "    \n",
    "        if len(rgb_image_list) == sequence_length and len(d_image_list) == sequence_length:\n",
    "            rgb_sequence_list.append(rgb_image_list) \n",
    "            d_sequence_list.append(d_image_list) \n",
    "            csgm_list.append(the_class)\n",
    "        \n",
    "    x_rgb = np.array(rgb_sequence_list)\n",
    "    print(x_rgb.shape)\n",
    "    x_d = np.array(d_sequence_list)\n",
    "    y = np.array(csgm_list)\n",
    "    return x_rgb, x_d, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn(input_shape, keys=[0,1,2], filter_info={0:[32,3]}, dropout={0:0.25}, pooling={0:2}, activation='relu', loss='mean_squared_error', final_activation='sigmoid'):    \n",
    "    \n",
    "    model = models.Sequential() \n",
    "    for k in keys:\n",
    "        num_filters = filter_info[k][0]\n",
    "        filter_size = filter_info[k][1]\n",
    "        if k == keys[0]: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation = activation, input_shape=input_shape))\n",
    "        else: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation= activation))\n",
    "                \n",
    "        if k in pooling.keys():           \n",
    "            pool_filter_size = pooling[k]\n",
    "            model.add(MaxPooling2D(pool_size=(pool_filter_size, pool_filter_size)))            \n",
    "                 \n",
    "        if k in dropout.keys(): \n",
    "            drop_rate = dropout[k]\n",
    "            model.add(Dropout(drop_rate))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 10, 480, 640, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 10, 480, 640, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 10, 58, 78, 1 93248       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 10, 58, 78, 1 93248       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 10, 58, 78, 2 0           time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 8, 56, 76, 32 221216      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)  (None, 4, 28, 38, 32 0           conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 136192)       0           max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          17432704    flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            129         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,840,545\n",
      "Trainable params: 17,840,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keys=[0,1,2]\n",
    "filter_info={0:[32,3],1:[64,3],2:[128,3]}\n",
    "dropout={0:0.25,1:0.25,2:0.25}\n",
    "pooling={0:2,1:2,2:2}\n",
    "\n",
    "rgb_model = create_cnn((480,640,3),\n",
    "                    keys,\n",
    "                    filter_info=filter_info,\n",
    "                    dropout=dropout,\n",
    "                    pooling=pooling,\n",
    "                    loss='mean_squared_error', \n",
    "                    final_activation='sigmoid',\n",
    "                    activation='relu')\n",
    "\n",
    "depth_model = create_cnn((480,640,3),\n",
    "                    keys,\n",
    "                    filter_info=filter_info,\n",
    "                    dropout=dropout,\n",
    "                    pooling=pooling,\n",
    "                    activation='relu')\n",
    "\n",
    "\n",
    "rgb_input = Input(shape=(sequence_length,480,640,3))\n",
    "encoded_rgb = TimeDistributed(rgb_model)(rgb_input)\n",
    "        \n",
    "depth_input = Input(shape=(sequence_length,480,640,3))\n",
    "encoded_depth = TimeDistributed(depth_model)(depth_input)\n",
    "        \n",
    "merged = keras.layers.concatenate([encoded_rgb, encoded_depth],axis=4)\n",
    "conv3d_1 = Conv3D(32,(3,3,3))(merged)\n",
    "max3d_1 = MaxPooling3D((2,2,2))(conv3d_1)\n",
    "flatten = Flatten()(max3d_1)\n",
    "dense = Dense(128, activation='softmax')(flatten)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "        \n",
    "\n",
    "model = Model(inputs=[rgb_input, depth_input], outputs=output)\n",
    "\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer=optimizers.SGD(lr=1e-4),\n",
    "              metrics=['acc']) \n",
    "\n",
    "#Start the new output file\n",
    "with open(statistics_output_file, 'w') as f:\n",
    "    if 'CUDA_VISIBLE_DEVICES' not in os.environ or os.environ['CUDA_VISIBLE_DEVICES'] == None or os.environ['CUDA_VISIBLE_DEVICES'] == '-1':\n",
    "        f.write('Used GPU: False\\n')\n",
    "    else:\n",
    "        f.write('Used GPU: True\\n')\n",
    "    f.write(\"where_clause:\\\"{}\\\"\\n\".format(where_clause))\n",
    "    f.write(\"limit_clause:\\\"{}\\\"\\n\".format(limit_clause))\n",
    "    f.write(\"epochs:\\\"{}\\\"\\n\".format(epochs))\n",
    "    f.write(\"batch_size:\\\"{}\\\"\\n\".format(batch_size))\n",
    "    f.write(\"max_accuracy:\\\"{}\\\"\\n\\n\".format(max_accuracy))\n",
    "    model.summary(print_fn=lambda x: f.write(\"Model:\\n{}\\n\".format(x)))\n",
    "\n",
    "model.summary() #output for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(exp_values, predicted_values):\n",
    "    \"\"\"\n",
    "    This creates a confusion matrix with the predicted accuracy of the model.\n",
    "    \n",
    "    exp_values must be in the format of a list and predicted values is expected to come in the format of the ouput \n",
    "    of Keras's model.predict()\n",
    "    \n",
    "    The ouput is a pandas dataframe that displays a confusion matrix indicitive of the accuracy of the model along \n",
    "        with a number score which is the accuracy of the model.\n",
    "    \"\"\"\n",
    "    predicted_values = convert_predictions(predicted_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Creates a DataFrame of zeros\n",
    "    matrix = pd.DataFrame(np.zeros((2,2)) , ['P0','P1'], ['E0','E1'])\n",
    "   \n",
    "    #Caculates whether the score was right or wrong and updates the confusion matrix \n",
    "    for i in range(len(exp_values)):\n",
    "        if exp_values[i] == predicted_values[i]:\n",
    "            matrix.iloc[[predicted_values[i]],[predicted_values[i]]] += 1\n",
    "        else:\n",
    "            matrix.iloc[[predicted_values[i]],[exp_values[i]]] += 1\n",
    "   \n",
    "    #Calculate diagonal sum and the accuracy of the model\n",
    "    #Precision (TP/TP+FPos)      Recall TP(TP+FNegative)\n",
    "    diagonal_sum = 0\n",
    "    for i in range(2):\n",
    "        diagonal_sum += matrix.iloc[i][i]\n",
    "    \n",
    "    score = diagonal_sum/len(exp_values)\n",
    "    \n",
    "  \n",
    "    return  matrix, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#updated\n",
    "def runTest(model,pooling, dropout, filter_info, file_name='model.txt', model_name='model', save_model=False, epochs=5, batch_size=32,max_accuracy=0.90):\n",
    "    \n",
    "    conn = connect()\n",
    "    try:\n",
    "        #recording_ids_dict = import_recording_ids(cache_path,conn)\n",
    "        class_data = import_class_data(conn)\n",
    "        \n",
    "        all_classes = class_data.keys()\n",
    "\n",
    "        matrices = {}\n",
    "        scores = {}\n",
    "        model_scores = {}\n",
    "    \n",
    "        #Set up to do cross-validation\n",
    "        all_recording_ids = set()\n",
    "        for a_class in all_classes:\n",
    "            for recording_id in class_data[a_class].keys():\n",
    "                all_recording_ids.add(recording_id)\n",
    "\n",
    "        #Run one fold of cross validation\n",
    "        for test_recording_id in all_recording_ids:\n",
    "            #Clean up some memory by garbage collecting variables that hold data\n",
    "            train_sequences = train_sequences_list = test_sequences = x_train = y_train = x_test = y_test = None\n",
    "            gc.collect()\n",
    "        \n",
    "            print('Testing on {0:3} '.format(test_recording_id),end=\"\")\n",
    "            train_sequences = list()\n",
    "            test_sequences = list()\n",
    "\n",
    "        \n",
    "            #Given the recording we are using for testing, split up the raw_ids into training and testing groups\n",
    "            for recording_id in all_recording_ids:\n",
    "                sequences = list()\n",
    "                for a_class in all_classes:\n",
    "                    for key, values in class_data[a_class].items():\n",
    "                        if key == recording_id:\n",
    "                            for value in values:\n",
    "                                sequences.append(value)\n",
    "                \n",
    "                if test_recording_id == recording_id:\n",
    "                    test_sequences.extend(sequences[:2])\n",
    "                else: \n",
    "                    train_sequences.extend(sequences[:2])\n",
    "                    \n",
    "            #Create the testing set\n",
    "            x_rgb_test, x_depth_test, y_test = create_array(conn,class_data,test_sequences)\n",
    "            print()\n",
    "            \n",
    "        \n",
    "            #Scaling the values to a value between 0 and 1\n",
    "            x_rgb_test = x_rgb_test.astype('float32')\n",
    "            x_rgb_test /= 255\n",
    "            \n",
    "            x_depth_test = x_depth_test.astype('float32')\n",
    "            x_depth_test /= 255\n",
    "        \n",
    "            epoch = 0\n",
    "            accuracy = 0\n",
    "            no_improvement = 0\n",
    "            while epoch < epochs and accuracy < max_accuracy and no_improvement < 2:\n",
    "                #Split the train_ids up into batches that are randomly shuffled and iteratively train themodel\n",
    "                train_sequences_list = list(train_sequences)\n",
    "                random.shuffle(train_sequences_list)\n",
    "                \n",
    "                print(\"\\tEpoch {0:5} {1:6} samples \".format(epoch,len(train_ids_list)),end=\"\")\n",
    "                \n",
    "                #Go through a batch at a time\n",
    "                for i in range(0,len(train_sequences_list), batch_size):\n",
    "                    #Get the current batch\n",
    "                    batch = list()\n",
    "                    for j in range(i,i+batch_size):\n",
    "                        if j < len(train_sequences_list):\n",
    "                            batch.append(train_sequences_list[j])\n",
    "                    \n",
    "                    #Fetch the images for the batch\n",
    "                    print(\" {} \".format(i),end=\"\",flush=True)\n",
    "                    x_rgb, x_depth, y_batch = create_array(conn,class_data,batch)\n",
    "                \n",
    "                    #Scaling the rgb values to a value between 0 and 1\n",
    "                    x_rgb = x_rgb.astype('float32')\n",
    "                    x_rgb /= 255\n",
    "                    x_depth = x_depth.astype('float32')\n",
    "                    x_depth /= 255\n",
    "                \n",
    "                    print('Shape of' + str(x_rgb.shape))\n",
    "                    print('Shape of' + str(x_depth.shape))\n",
    "                    #Fit the model\n",
    "                    model.train_on_batch([x_rgb ,x_depth], y_batch)\n",
    "                print(\"\",flush=True)\n",
    "              \n",
    "                #Create predictions and evaluate to find loss and accuaracy\n",
    "                predict = model.predict([x_rgb_test,x_depth_test])\n",
    "                model_score = model.evaluate([x_rgb_test,x_depth_test], y_test)\n",
    "                print('\\t\\tModel was ' + str(model_score[1]) + '% accurate and exhibited an average loss of ' + str(model_score[0]) + '.')\n",
    "\n",
    "                matrix,score = confusion_matrix(y_test, predict)\n",
    "                print(str(matrix) + '\\n')\n",
    "                print(str(score) + '\\n')\n",
    "\n",
    "                results.append({\"matrix\":matrix,\n",
    "                             \"score\":score,\n",
    "                             \"model_score\":model_score\n",
    "                            })\n",
    "\n",
    "                #update epoch\n",
    "                epoch = epoch + 1\n",
    "\n",
    "                #update loss\n",
    "                loss = model_score[0]\n",
    "\n",
    "                #if accuracy didn't change...\n",
    "                if accuracy == model_score[1]:\n",
    "                    delta_loss_percentage = abs(loss - model_score[0])/loss\n",
    "                    average_delta_loss_percentage = 0.9*average_delta_loss_percentage + 0.1*delta_loss_percentage\n",
    "                else:\n",
    "                    average_delta_loss_percentage = 1.0\n",
    "\n",
    "                #update accuracy\n",
    "                accuracy = model_score[1]\n",
    "\n",
    "            if epoch == epochs:\n",
    "                stop_reason = \"Reached end of training run\"\n",
    "                print(\"Reached end of training run\")\n",
    "            if accuracy >= max_accuracy:\n",
    "                stop_reason = \"Stopped training early because max_accuracy was reached\"\n",
    "            if average_delta_loss_percentage < loss_threshold:\n",
    "                stop_reason = \"Training stopped progressing\"\n",
    "\n",
    "            print(stop_reason)\n",
    "            output_file.write(\"{},\".format(accuracy))\n",
    "            output_file.write(\"{},\".format(loss))\n",
    "            output_file.write(\"{},\".format(matrix.iloc[0][0]))\n",
    "            output_file.write(\"{},\".format(matrix.iloc[0][1]))\n",
    "            output_file.write(\"{},\".format(matrix.iloc[1][0]))\n",
    "            output_file.write(\"{},\".format(matrix.iloc[1][1]))  \n",
    "            output_file.write(\"{}\\n\".format(stop_reason))\n",
    "        aggregate_accuracy = []\n",
    "        aggregate_loss = []\n",
    "        aggregate_TN = []\n",
    "        aggregate_FN = []\n",
    "        aggregate_FP = []\n",
    "        aggregate_TP = []\n",
    "        for r in results:\n",
    "            aggregate_accuracy.append(r[\"model_score\"][1])\n",
    "            aggregate_loss.append(r[\"model_score\"][0])\n",
    "            aggregate_TN.append(r[\"matrix\"].iloc[0][0])\n",
    "            aggregate_FN.append(r[\"matrix\"].iloc[0][1])\n",
    "            aggregate_FP.append(r[\"matrix\"].iloc[1][0])\n",
    "            aggregate_TP.append(r[\"matrix\"].iloc[1][1])\n",
    "        output_file.write(\"Across all folds:\\n\")\n",
    "        output_file.write(\"Average Accuracy: {}, Standard Deviation: {}\\n\".format(statistics.mean(aggregate_accuracy),statistics.stdev(aggregate_accuracy)))\n",
    "        output_file.write(\"Average Loss: {}, Standard Deviation: {}\\n\".format(statistics.mean(aggregate_loss),statistics.stdev(aggregate_loss)))\n",
    "        output_file.write(\"\\t\\t\\tEO\\tE1\\n\")\n",
    "        output_file.write(\"\\tP0\\t{}\\t{}\\n\".format(statistics.sum(aggregate_TN),statistics.sum(aggregate_FN)))\n",
    "        output_file.write(\"\\tP0\\t{}\\t{}\\n\".format(statistics.sum(aggregate_FP),statistics.sum(aggregate_TP)))\n",
    "\n",
    "        if save_model: \n",
    "            model.save(model)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on   2 oooooooooooooooooooo(2, 10, 480, 640, 3)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-230260377c67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         max_accuracy=max_accuracy)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-c438b4d8552f>\u001b[0m in \u001b[0;36mrunTest\u001b[0;34m(model, pooling, dropout, filter_info, file_name, model_name, save_model, epochs, batch_size, max_accuracy)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tEpoch {0:5} {1:6} samples \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m#Go through a batch at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "runTest(model=model, file_name=statistics_output_file, \n",
    "        filter_info=filter_info,  \n",
    "        dropout=dropout, \n",
    "        pooling=pooling, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        max_accuracy=max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
