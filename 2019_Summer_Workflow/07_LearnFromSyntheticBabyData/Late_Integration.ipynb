{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import from the Keras library\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten, Input \n",
    "from keras.layers import Conv2D,  MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import optimizers \n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "#from secret import credentials\n",
    "\n",
    "#This allows for Keras models to be saved. \n",
    "import h5py\n",
    "#Other import statements \n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import cv2\n",
    "import pymysql\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Secrets shouldn't be in the repository\n",
    "from secrets import credentials\n",
    "\n",
    "\n",
    "def connect(): \n",
    "    db_host = credentials['db_host'];\n",
    "    db_port = credentials['db_port'];\n",
    "    db_name = credentials['db_name'];\n",
    "    db_username = credentials['db_username']\n",
    "    db_password = credentials['db_password']\n",
    "    \n",
    "    conn = pymysql.connect(db_host, user=db_username, port=db_port, passwd=db_password, db=db_name)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_data(cache_path, conn=connect()): \n",
    "\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    recording_ids = {}\n",
    "    xy = {}    \n",
    "        \n",
    "    image_query = \"SELECT r.id, r.recording_id, r.isCSGM FROM nicu.Video_Raw AS r JOIN nicu.Video_Generated AS g ON r.id=g.raw_id  WHERE (r.recording_id>1) AND (g.RGB_Optical_Flow IS NOT NULL) LIMIT 1500\"\n",
    "    try:\n",
    "        curs.execute(image_query) #(list(recording_ids.keys())))\n",
    "        for row in curs.fetchall():\n",
    "            raw_id = row[0]\n",
    "            rec_id = row[1]\n",
    "            csgm = row[2]\n",
    "            if rec_id in recording_ids:\n",
    "                recording_ids.get(rec_id).append(raw_id)\n",
    "            else:\n",
    "                recording_ids.update({rec_id:[raw_id]})\n",
    "            xy.update({raw_id:[csgm]})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving ID's\", e)\n",
    "        raise e\n",
    "    \n",
    "    #cache_path = cache_path+\"recording_\"+(\"{:02d}\".format(rec_id))\n",
    "    if not os.path.exists(cache_path):\n",
    "        os.mkdir(cache_path)\n",
    "\n",
    "    cache_path = cache_path+'testing'   \n",
    "    for rec_id in recording_ids:\n",
    "        \n",
    "        #I don't know what this code is doing? - RK\n",
    "        '''\n",
    "        #Temporarily shortens the amount of data to use to be able to test\n",
    "        raw_id_list = recording_ids.get(rec_id)\n",
    "        recording_ids.update({rec_id:raw_id_list})\n",
    "        '''\n",
    "        for raw_id in recording_ids.get(rec_id):\n",
    "            rgb_path = cache_path+'/'+str(raw_id)+\".oflow.png\"\n",
    "            depth_path = cache_path+'/'+str(raw_id)+\".dflow.png\"\n",
    "            if not (os.path.exists(rgb_path) and os.path.exists(depth_path)):\n",
    "                try:\n",
    "                    image_query = \"SELECT RGB_Optical_Flow, D_Depth_Flow from Video_Generated WHERE (raw_id=%s)\"\n",
    "                    curs.execute(image_query, (str(raw_id)))\n",
    "                    for row in curs.fetchall():\n",
    "                        rgb_img = row[0]\n",
    "                        depth_img = row[1]\n",
    "                        if (rgb_img is not None) and not(os.path.exists(rgb_path)):\n",
    "                            rgb_cv = cv2.imdecode(np.asarray(bytearray(rgb_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                            cv2.imwrite(rgb_path,rgb_cv)\n",
    "                        if depth_img is not None and not(os.path.exists(depth_path)):\n",
    "                            depth_cv = cv2.imdecode(np.asarray(bytearray(depth_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                            cv2.imwrite(depth_path,depth_cv)\n",
    "                except Exception as e:\n",
    "                        print(\"Error retrieving Optical Flow frame\",e)\n",
    "                        curs.close();\n",
    "                        raise e\n",
    "            \n",
    "            #Only converts the two images and adds them to the containers if both exist.\n",
    "            if os.path.exists(rgb_path) and os.path.exists(depth_path):\n",
    "                rgb_img = create_image(rgb_path, scale)\n",
    "                depth_img = create_image(depth_path, scale)\n",
    "                xy.get(raw_id).extend([rgb_img,depth_img])\n",
    "    curs.close()\n",
    "    \n",
    "    return recording_ids, xy\n",
    "\n",
    "def create_image(path, scale=100):\n",
    "    '''\n",
    "    Creates and resizes a cv2 image to scale/100\n",
    "    '''\n",
    "    img = cv2.imread(path)\n",
    "    width = int(img.shape[1] * (scale / 100))\n",
    "    height = int(img.shape[0] * (scale / 100))\n",
    "    img = cv2.resize(img,(width,height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_array(raw_ids, xy):    \n",
    "    \"\"\"\n",
    "    This takes a list of ids in str form and a dictionary with keys of ids in str form with values of lists containing\n",
    "    the rgb in cv2 form at index 1 the depth flow in cv2 at index 2 and the int 1 or 0 at index 0 which identifies whether \n",
    "    these images exhibit CSGM movement. \n",
    "    \n",
    "    Output: Three numpy arrays\n",
    "    \"\"\"\n",
    "    rgb_list=[]\n",
    "    depth_list=[]\n",
    "    csgm_list=[]\n",
    "    \n",
    "    #shuffles the ids for randomness\n",
    "    random.shuffle(raw_ids)\n",
    "    \n",
    "    for i in raw_ids:\n",
    "        if len(xy.get(i)) == 3:\n",
    "            #if not xy.get(i)[0] == None:\n",
    "            rgb_list.append(xy.get(i)[1])\n",
    "            depth_list.append(xy.get(i)[2])\n",
    "            csgm_list.append(xy.get(i)[0])\n",
    "    x_rgb = np.array(rgb_list)\n",
    "    x_depth = np.array(depth_list)\n",
    "    y = np.array(csgm_list)\n",
    "    return x_rgb, x_depth, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn(x_train, filter_info={0:[32,3]}, dropout={0:0.25}, pooling={0:2}, activation='relu'):\n",
    "    \"\"\"\n",
    "    Input: The only required input is the array of input data. \n",
    "    \n",
    "    Outpu: Outputs a keras Sequential model and a str representation of that model. \n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    str_model = \"Overview of Model Architecture: \\n\"\n",
    "    \n",
    "    filter_size = 0\n",
    "    \n",
    "    for i in filter_info: \n",
    "        filter_size = filter_info.get(i)[1]\n",
    "        num_filters = filter_info.get(i)[0]\n",
    "        \n",
    "        if i == 0: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation = 'relu', input_shape=x_train.shape[1:]))\n",
    "        else: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation= 'relu'))\n",
    "        \n",
    "        str_model += (\"2D Convulution Layer with %d filters the size of (%d,%d) and %s activation \\n\" %(num_filters, filter_size, filter_size, activation))\n",
    "        \n",
    "        model.add(Conv2D(num_filters, (filter_size,filter_size), activation= 'relu'))\n",
    "        str_model += (\"2D Convulution Layer with %d filters the size of (%d,%d) and %s activation \\n\" %(num_filters, filter_size, filter_size, activation))\n",
    "        \n",
    "        if i in pooling:           \n",
    "            pool_filter_size = pooling.get(i)\n",
    "            model.add(MaxPooling2D(pool_size=(pool_filter_size, pool_filter_size)))\n",
    "            str_model += ('2D Pooling Max Pooling Layer with filter size (%d,%d)\\n' %(pool_filter_size,pool_filter_size))\n",
    "            \n",
    "                 \n",
    "        if i in dropout: \n",
    "            drop_rate = dropout.get(i)\n",
    "            model.add(Dropout(drop_rate))\n",
    "            str_model += ('Droput Layer with with a rate of %f \\n' %(drop_rate))\n",
    "\n",
    "\n",
    "    \n",
    "    #These will be added to the end of every model no matter what\n",
    "    model.add(Flatten())\n",
    "    str_model += ('Flatten\\n')                 \n",
    "    print(str_model)\n",
    "\n",
    "    return model, str_model\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(exp_values, predicted_values):\n",
    "    \"\"\"\n",
    "    This creates a confusion matrix with the predicted accuracy of the model.\n",
    "    \n",
    "    exp_values must be in the format of a list and predicted values is expected to come in the format of the ouput \n",
    "    of Keras's model.predict()\n",
    "    \n",
    "    The ouput is a pandas dataframe that displays a confusion matrix indicitive of the accuracy of the model along \n",
    "        with a number score which is the accuracy of the model.\n",
    "    \"\"\"\n",
    "    predicted_values = convert_predictions(predicted_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Creates a DataFrame of zeros\n",
    "    matrix = pd.DataFrame(np.zeros((2,2)) , ['P0','P1'], ['E0','E1'])\n",
    "   \n",
    "    #Caculates whether the score was right or wrong and updates the confusion matrix \n",
    "    for i in range(len(exp_values)):\n",
    "        if exp_values[i] == predicted_values[i]:\n",
    "            matrix.iloc[[predicted_values[i]],[predicted_values[i]]] += 1\n",
    "        else:\n",
    "            matrix.iloc[[predicted_values[i]],[exp_values[i]]] += 1\n",
    "   \n",
    "    #Calculate diagonal sum and the accuracy of the model\n",
    "    diagonal_sum = 0\n",
    "    for i in range(2):\n",
    "        diagonal_sum += matrix.iloc[i][i]\n",
    "    \n",
    "    score = diagonal_sum/len(exp_values)\n",
    "    \n",
    "    return  matrix, score\n",
    "    \n",
    "    \n",
    "            \n",
    "def convert_predictions(predictions): \n",
    "    \"\"\"\n",
    "    Converts predictions outputted by a keras model into a list with 1 represented the predicted output and zero \n",
    "    in other classes. \n",
    "    \"\"\"\n",
    "    l =[]\n",
    "    max_prediction = 0\n",
    "    \n",
    "    #Finds the highest prediction\n",
    "    for p in predictions:\n",
    "        if p > max_prediction:\n",
    "            max_prediction = p\n",
    "    \n",
    "    #Scales predictions and determines if it is 1 or 0\n",
    "    for p in predictions: \n",
    "        p = p/max_prediction\n",
    "        if p >= 0.5:\n",
    "            l.append(1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest(pooling, dropout, filter_info, loss, activation, final_activation, file_name='model.txt', model_name='model', save_model=False, epochs=5, batch_size=32):\n",
    "    #Dr. Patterson - you will need to update this line of code for it to work in your directory\n",
    "    recording_ids_dict, xy = import_data('/Users/jonathanlee/Desktop/Python/NICU/NICU_data')\n",
    "\n",
    "    matrices = {}\n",
    "    scores = {}\n",
    "    model_scores = {}\n",
    "    str_model =''\n",
    "\n",
    "    #This essentially uses KFold cross validation using every recording Id as a new fold\n",
    "    for i in recording_ids_dict:\n",
    "        print('Testing on ' + str(i))\n",
    "        train_ids = numpy.concatenate(recording_ids.values())\n",
    "        test_ids = recording_ids_dict[i]\n",
    "\n",
    "        #This should run over far less records than re-iterating over the entire dict of ids        \n",
    "        for j in train_ids:\n",
    "            train_ids.remove(j)\n",
    "        \n",
    "        #This creates an even amount of examples of CSGM and not CSGM\n",
    "        num_is = 0\n",
    "        num_isnt = 0\n",
    "        is_ids = []\n",
    "        \n",
    "        for j in train_ids:\n",
    "            if xy.get(j)[0] == 0: \n",
    "                num_isnt += 1\n",
    "            else:\n",
    "                num_is += 1\n",
    "                is_ids.append(j)\n",
    "            \n",
    "        print('Num is %d and num isn\\'t %d' %(num_is, num_isnt))\n",
    "        for j in range(num_isnt-num_is): \n",
    "            train_ids.append(is_ids[(j % len(is_ids))])\n",
    "            \n",
    "        print('Length ' + str(len(train_ids))) \n",
    "            \n",
    "            \n",
    "            \n",
    "        x_rgb_train, x_depth_train, y_train = create_array(train_ids, xy)\n",
    "        x_rgb_test, x_depth_test, y_test = create_array(test_ids, xy)\n",
    "        \n",
    "        #Scaling the values to a value between 0 and 1\n",
    "        x_rgb_train = x_rgb_train.astype('float32')\n",
    "        x_rgb_test = x_rgb_test.astype('float32')\n",
    "        x_rgb_train /= 255\n",
    "        x_rgb_test /= 255\n",
    "        x_depth_train = x_depth_train.astype('float32')\n",
    "        x_depth_test = x_depth_test.astype('float32')\n",
    "        x_depth_train /= 255\n",
    "        x_depth_test /= 255\n",
    "        \n",
    "        depth_model,depth_str_model = create_cnn(x_depth_train,\n",
    "                                     filter_info=filter_info,\n",
    "                                     dropout=dropout,\n",
    "                                     pooling=pooling,\n",
    "                                     activation=activation)\n",
    "        \n",
    "        rgb_model,rgb_str_model = create_cnn(x_rgb_train,\n",
    "                                     filter_info=filter_info,\n",
    "                                     dropout=dropout,\n",
    "                                     pooling=pooling,\n",
    "                                     activation=activation)\n",
    "       \n",
    "        \n",
    "        #The rest of the model empoys the functional api of Keras\n",
    "        rgb_input = Input(shape=(x_rgb_train.shape[1:]))\n",
    "        encoded_rgb = rgb_model(rgb_input)\n",
    "        \n",
    "        depth_input = Input(shape=(x_depth_train.shape[1:]))\n",
    "        encoded_depth = depth_model(depth_input)\n",
    "        \n",
    "        \n",
    "        merged = keras.layers.concatenate([encoded_rgb, encoded_depth])\n",
    "        a = Dense(256)(merged)\n",
    "        b = Dense(512)(a)\n",
    "        #Fit the model\n",
    "        output = Dense(1, activation=final_activation)(b)\n",
    "        \n",
    "        model = Model(inputs=[rgb_input, depth_input], outputs=output)\n",
    "        \n",
    "        model.compile(optimizer='sgd',\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "        model.fit([x_rgb_train, x_depth_train], \n",
    "                  [y_train],\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size)\n",
    "        \n",
    "    \n",
    "        #Create predictions and evaluate to find loss and accuaracy\n",
    "        model_score = model.evaluate(x =[x_rgb_test, x_depth_test], y=y_test)\n",
    "        predict = model.predict([x_rgb_test, x_depth_test])\n",
    "        print(predict)\n",
    "        print('Model was ' + str(model_score[1]) + '% accurate and exhibited an average loss of ' + str(model_score[0]) + '.')\n",
    "        \n",
    "        matrix,score = confusion_matrix(y_test, predict)\n",
    "        \n",
    "        matrices.update({i : matrix})\n",
    "        print(str(matrix) + '\\n')\n",
    "        scores.update({i: score})\n",
    "        print(str(score) + '\\n')\n",
    "        model_scores.update({i:model_score})\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    matrix = pd.DataFrame(np.zeros((2,2)) , ['P0','P1'], ['E0','E1'])\n",
    "    \n",
    "    \"\"\"\n",
    "    Visulaization of the df\n",
    "       E0 E1\n",
    "    P0\n",
    "    P1\n",
    "    \"\"\"\n",
    "    \n",
    "    num_predictions = 0\n",
    "    \n",
    "    for i in range(2): \n",
    "        for j in range(2):\n",
    "            sum_temp = 0\n",
    "            for baby in matrices: \n",
    "                matrix.iloc[[i],[j]] += matrices.get(baby).iloc[i][j]\n",
    "                num_predictions += matrices.get(baby).iloc[i][j]\n",
    "                \n",
    "                \n",
    "    diagonal_sum = 0\n",
    "    for i in range(2):\n",
    "        diagonal_sum += matrix.iloc[i][i]\n",
    "        \n",
    "    final_score = diagonal_sum/num_predictions\n",
    "    \n",
    "    #Holding that CSGM is positive and not CSGM is negative\n",
    "    true_pos = matrix['E1']['P1'] + matrix['E0']['P0']\n",
    "    false_pos = matrix['E0']['P1']  \n",
    "    false_negative = matrix['E1']['P0']\n",
    "    \n",
    "    precision = true_pos/(true_pos+false_pos)\n",
    "    recall =  true_pos/(true_pos+false_negative)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    with open(file_name, 'w') as f:\n",
    "        for key in matrices:\n",
    "            f.write(\"Baby %s\\n\" % key)\n",
    "            f.write(\"%s\\n\" % str_model)\n",
    "            f.write(\"%s\\n\" % matrices[key])\n",
    "            f.write(\"%s\\n\" % scores[key])\n",
    "            f.write(\"%s\\n\" % model_scores[key])\n",
    "        f.write('Cumlative Statistics\\n')\n",
    "        f.write(\"%s\\n\" % matrix)\n",
    "        f.write('Final score is %s\\n' % final_score)\n",
    "        f.write('Precision of this model is %s\\n' % precision)\n",
    "        f.write('Recall of this model is %s\\n' % recall)\n",
    "        \n",
    "            \n",
    "    if save_model : \n",
    "        model.save(model)\n",
    "    #Add a final matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 2\n",
      "Num is 23 and num isn't 27\n",
      "Length 54\n",
      "Overview of Model Architecture: \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Pooling Max Pooling Layer with filter size (2,2)\n",
      "Droput Layer with with a rate of 0.250000 \n",
      "Flatten\n",
      "\n",
      "Overview of Model Architecture: \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Pooling Max Pooling Layer with filter size (2,2)\n",
      "Droput Layer with with a rate of 0.250000 \n",
      "Flatten\n",
      "\n",
      "Epoch 1/1\n",
      "53/53 [==============================] - 8s 151ms/step - loss: 0.6932 - acc: 0.3962\n",
      "49/49 [==============================] - 1s 29ms/step\n",
      "[[0.5051947 ]\n",
      " [0.50510025]\n",
      " [0.5072637 ]\n",
      " [0.5048847 ]\n",
      " [0.50715727]\n",
      " [0.5051833 ]\n",
      " [0.50573593]\n",
      " [0.5058228 ]\n",
      " [0.50674194]\n",
      " [0.505481  ]\n",
      " [0.51400554]\n",
      " [0.5053894 ]\n",
      " [0.5027432 ]\n",
      " [0.5040253 ]\n",
      " [0.508105  ]\n",
      " [0.5042448 ]\n",
      " [0.50559807]\n",
      " [0.5083286 ]\n",
      " [0.50626665]\n",
      " [0.50463253]\n",
      " [0.5074105 ]\n",
      " [0.5070746 ]\n",
      " [0.50418276]\n",
      " [0.5055795 ]\n",
      " [0.50346094]\n",
      " [0.5055504 ]\n",
      " [0.50612247]\n",
      " [0.50658786]\n",
      " [0.50476146]\n",
      " [0.50909   ]\n",
      " [0.50504375]\n",
      " [0.5041768 ]\n",
      " [0.5078351 ]\n",
      " [0.50405264]\n",
      " [0.50665253]\n",
      " [0.5120808 ]\n",
      " [0.50494426]\n",
      " [0.5101533 ]\n",
      " [0.508122  ]\n",
      " [0.50569695]\n",
      " [0.5033632 ]\n",
      " [0.50612247]\n",
      " [0.5069752 ]\n",
      " [0.51109946]\n",
      " [0.50673926]\n",
      " [0.50962114]\n",
      " [0.5061957 ]\n",
      " [0.5069894 ]\n",
      " [0.50636894]]\n",
      "Model was 0.0612244898719447% accurate and exhibited an average loss of 0.7045225634866831.\n",
      "      E0   E1\n",
      "P0   0.0  0.0\n",
      "P1  46.0  3.0\n",
      "\n",
      "0.061224489795918366\n",
      "\n",
      "Testing on 3\n",
      "Num is 3 and num isn't 47\n",
      "Length 94\n",
      "Overview of Model Architecture: \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Pooling Max Pooling Layer with filter size (2,2)\n",
      "Droput Layer with with a rate of 0.250000 \n",
      "Flatten\n",
      "\n",
      "Overview of Model Architecture: \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Convulution Layer with 32 filters the size of (3,3) and relu activation \n",
      "2D Pooling Max Pooling Layer with filter size (2,2)\n",
      "Droput Layer with with a rate of 0.250000 \n",
      "Flatten\n",
      "\n",
      "Epoch 1/1\n",
      "93/93 [==============================] - 10s 105ms/step - loss: 0.6763 - acc: 0.7957\n",
      "49/49 [==============================] - 2s 32ms/step\n",
      "[[0.507469  ]\n",
      " [0.49812022]\n",
      " [0.50638455]\n",
      " [0.4989221 ]\n",
      " [0.505034  ]\n",
      " [0.5014079 ]\n",
      " [0.5013854 ]\n",
      " [0.49563634]\n",
      " [0.50155973]\n",
      " [0.4995483 ]\n",
      " [0.50174975]\n",
      " [0.49552304]\n",
      " [0.5006754 ]\n",
      " [0.5063791 ]\n",
      " [0.497151  ]\n",
      " [0.50062484]\n",
      " [0.5033508 ]\n",
      " [0.51519316]\n",
      " [0.5060436 ]\n",
      " [0.49952635]\n",
      " [0.50968534]\n",
      " [0.5001553 ]\n",
      " [0.49959356]\n",
      " [0.5047705 ]\n",
      " [0.5099616 ]\n",
      " [0.521637  ]\n",
      " [0.50262696]\n",
      " [0.51618636]\n",
      " [0.4995493 ]\n",
      " [0.49494627]\n",
      " [0.50100255]\n",
      " [0.49746197]\n",
      " [0.5207034 ]\n",
      " [0.5035898 ]\n",
      " [0.49396643]\n",
      " [0.49693513]\n",
      " [0.5033876 ]\n",
      " [0.49420762]\n",
      " [0.50727016]\n",
      " [0.50153446]\n",
      " [0.5028953 ]\n",
      " [0.5042127 ]\n",
      " [0.50317323]\n",
      " [0.49536964]\n",
      " [0.49550202]\n",
      " [0.5041701 ]\n",
      " [0.4950895 ]\n",
      " [0.5042526 ]\n",
      " [0.49507743]]\n",
      "Model was 0.7142857191513996% accurate and exhibited an average loss of 0.6865336457077338.\n",
      "      E0    E1\n",
      "P0   0.0   0.0\n",
      "P1  26.0  23.0\n",
      "\n",
      "0.46938775510204084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#File name for the statistics to be save in. Must include .txt at the end\n",
    "file_name = 'late_integration.txt'\n",
    "\n",
    "#Must have the h5py package installed or the model will not save. This should be the path of the location you would like\n",
    "#To save the model\n",
    "model_file_name = 'test'\n",
    "\n",
    "filter_info={0:[32,3]}\n",
    "dropout={0:0.25}\n",
    "pooling={0:2}\n",
    "\n",
    "\n",
    "runTest(file_name=file_name, \n",
    "        filter_info=filter_info, \n",
    "        dropout=dropout, \n",
    "        pooling=pooling, \n",
    "        loss='binary_crossentropy', \n",
    "        activation='relu',\n",
    "        epochs=1, \n",
    "        batch_size=16,\n",
    "        final_activation='sigmoid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
