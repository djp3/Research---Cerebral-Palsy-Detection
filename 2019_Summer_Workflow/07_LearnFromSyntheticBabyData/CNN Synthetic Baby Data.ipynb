{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "#Import from the Keras library\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D,  MaxPooling2D\n",
    "from keras import optimizers \n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from secret import credentials\n",
    "\n",
    "#This allows for Keras models to be saved. \n",
    "import h5py\n",
    "#Other import statements \n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import cv2\n",
    "import pymysql\n",
    "import os\n",
    "\n",
    "#pick which recordings you are working with\n",
    "where_clause = \"WHERE (r.recording_id > 1)\"\n",
    "#limit the data to a subset for testing\n",
    "#Make empty string to have no limit\n",
    "#limit = \"ORDER BY RAND(1) LIMIT 3000\"\n",
    "limit_clause = \"ORDER BY RAND(1)\"\n",
    "\n",
    "#What size should we reduce images to before learning in percent\n",
    "#image_scale_percent = 1\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "max_accuracy = 0.90 #Quit if accuracy reaches this at any epoch\n",
    "\n",
    "#directory where data from database is stored\n",
    "cache_path = 'cache'\n",
    "\n",
    "#File name for the statistics to be save in. Must include .txt at the end\n",
    "statistics_output_file = 'statistics.output.txt'\n",
    "\n",
    "#Must have the h5py package installed or the model will not save. This should be the path of the location you would like\n",
    "#To save the model\n",
    "model_file_name = 'model.output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Secrets shouldn't be in the repository\n",
    "from secrets import credentials\n",
    "# Of the form\n",
    "#credentials = {\n",
    "#        'db_host' : 'something.us-east-1.rds.amazonaws.com'\n",
    "#        'db_port' : 3306\n",
    "#        'db_name' : 'name',\n",
    "#        'db_username' : 'something',\n",
    "#        'db_password' : 'secret'\n",
    "#        }\n",
    "\n",
    "\n",
    "def connect(): \n",
    "    db_host = credentials['db_host'];\n",
    "    db_port = credentials['db_port'];\n",
    "    db_name = credentials['db_name'];\n",
    "    db_username = credentials['db_username']\n",
    "    db_password = credentials['db_password']\n",
    "    \n",
    "    conn = pymysql.connect(db_host, user=db_username, port=db_port, passwd=db_password, db=db_name)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_class_data(conn): \n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        #a dictionary that maps classes to a dictionary that maps recording_ids to a set of raw_ids\n",
    "        ''' 2 classes , 2 recording ids, 15 frames\n",
    "            {\n",
    "                0:{\n",
    "                    2:{1,2,3,4},\n",
    "                    3:{5,6,7,8}\n",
    "                },\n",
    "                1:{\n",
    "                    2:{9,10,11,12},\n",
    "                    3:{13,14,15}\n",
    "                }\n",
    "            }\n",
    "        '''\n",
    "                    \n",
    "        class_data={}\n",
    "        \n",
    "        recording_query = \"SELECT r.id, r.recording_id, r.isCSGM FROM nicu.Video_Raw AS r JOIN nicu.Video_Generated AS g ON r.id=g.raw_id \"+where_clause+\" \"+limit_clause\n",
    "        try:\n",
    "            cursor.execute(recording_query)\n",
    "            for row in cursor.fetchall():\n",
    "                raw_id = row[0]\n",
    "                rec_id = row[1]\n",
    "                csgm = row[2]\n",
    "                \n",
    "                if csgm not in class_data:\n",
    "                    class_data[csgm] = dict()\n",
    "                    \n",
    "                single_class_data = class_data[csgm]\n",
    "                \n",
    "                if rec_id not in single_class_data:\n",
    "                    single_class_data[rec_id] = set()\n",
    "                    \n",
    "                single_class_data[rec_id].add(raw_id)\n",
    "                                                      \n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving ID's\", e)\n",
    "            conn.rollback()\n",
    "            raise e\n",
    "            \n",
    "        return class_data\n",
    "    finally:\n",
    "        cursor.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_batch(cache_path,conn,recording_ids,page,batch_size=32):\n",
    "\n",
    "    #Create the cache directory if it doesn't exist\n",
    "    if not os.path.exists(cache_path):\n",
    "        os.mkdir(cache_path)\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        recording_ids = {}\n",
    "        xy = {}    \n",
    "        \n",
    "        image_query = \"SELECT r.id, r.recording_id, r.isCSGM FROM nicu.Video_Raw AS r JOIN nicu.Video_Generated AS g ON r.id=g.raw_id  WHERE (r.recording_id>1) AND (g.RGB_Optical_Flow IS NOT NULL) \" +limit\n",
    "        try:\n",
    "            cursor.execute(image_query) #(list(recording_ids.keys())))\n",
    "            for row in cursor.fetchall():\n",
    "                raw_id = row[0]\n",
    "                rec_id = row[1]\n",
    "                csgm = row[2]\n",
    "                if rec_id in recording_ids:\n",
    "                    recording_ids.get(rec_id).append(raw_id)\n",
    "                else:\n",
    "                    recording_ids.update({rec_id:[raw_id]})\n",
    "                xy.update({raw_id:[csgm]})\n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving ID's\", e)\n",
    "            conn.rollback()\n",
    "            raise e\n",
    "            \n",
    "        print(\"Collecting images for processing (o = source image in cache, ⇣ = source image fetched from db, x = source image not in db)\")\n",
    "        for rec_id in recording_ids:\n",
    "            print(\"\")\n",
    "            print(\"Analyzing recording_id:\",rec_id,\": \",end=\"\")\n",
    "            raw_id_list = recording_ids.get(rec_id)\n",
    "            for raw_id in raw_id_list:\n",
    "                current_input = cache_path+'/'+str(raw_id)+\".oflow.png\"\n",
    "                if not os.path.exists(current_input):\n",
    "                    cursor2 = conn.cursor()\n",
    "                    try:\n",
    "                        image_query = \"SELECT RGB_Optical_Flow from Video_Generated WHERE (raw_id=%s)\"\n",
    "                        cursor2.execute(image_query, (str(raw_id)))\n",
    "                        for row in cursor2.fetchall():\n",
    "                            db_img = row[0]\n",
    "                            if db_img is not None:\n",
    "                                img=cv2.imdecode(np.asarray(bytearray(db_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                                cv2.imwrite(current_input,img)\n",
    "                                print(\"⇣\",end=\"\",flush=True)\n",
    "                            else:\n",
    "                                print(\"x\",end=\"\",flush=True)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error retrieving Optical Flow frame\",e)\n",
    "                        raise e\n",
    "                    finally:\n",
    "                        cursor2.close()     \n",
    "                else:\n",
    "                    print(\"o\",end=\"\",flush=True)\n",
    "\n",
    "                #Resizing the image\n",
    "                img = cv2.imread(current_input)\n",
    "              \n",
    "                width = int(img.shape[1] * image_scale_percent / 100)\n",
    "                height = int(img.shape[0] * image_scale_percent / 100)\n",
    "                cv2.resize(img,(width,height), interpolation=cv2.INTER_CUBIC)\n",
    "                xy.get(raw_id).insert(0,img)\n",
    "        print(\"\")\n",
    "        return recording_ids, xy\n",
    "    finally:\n",
    "        cursor.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(conn, class_data, batch_raw_ids):     \n",
    "    \n",
    "    #Find the true class of the elements in the batch\n",
    "    raw_id_dict = {} #The raw_id mapped to the class\n",
    "    all_classes = class_data.keys()\n",
    "    for a_class in all_classes:\n",
    "        for recording_id, raw_ids in class_data[a_class].items():\n",
    "            for raw_id in raw_ids:\n",
    "                if raw_id in batch_raw_ids:\n",
    "                    raw_id_dict[raw_id] = a_class\n",
    "    \n",
    "    #Retrieve the images\n",
    "    image_list=[]\n",
    "    csgm_list=[]\n",
    "    \n",
    "    for raw_id,the_class in raw_id_dict.items():\n",
    "        #Check the cache for the image in question\n",
    "        current_input = cache_path+'/'+str(raw_id)+\".oflow.png\"\n",
    "        if not os.path.exists(current_input):\n",
    "            cursor = conn.cursor()\n",
    "            try:\n",
    "                image_query = \"SELECT RGB_Optical_Flow from Video_Generated WHERE (raw_id=%s)\"\n",
    "                cursor.execute(image_query, (str(raw_id)))\n",
    "                for row in cursor.fetchall():\n",
    "                    db_img = row[0]\n",
    "                    if db_img is not None:\n",
    "                        img=cv2.imdecode(np.asarray(bytearray(db_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                        cv2.imwrite(current_input,img)\n",
    "                        print(\"⇣\",end=\"\",flush=True)\n",
    "                    else:\n",
    "                        print(\"x\",end=\"\",flush=True)\n",
    "            except Exception as e:\n",
    "                print(\"Error retrieving Optical Flow frame\",e)\n",
    "                raise e\n",
    "            finally:\n",
    "                cursor.close()     \n",
    "        else:\n",
    "            print(\"o\",end=\"\",flush=True)\n",
    "\n",
    "        img = cv2.imread(current_input)\n",
    "         \n",
    "        #Resizing the image\n",
    "        #width = int(img.shape[1] * image_scale_percent / 100)\n",
    "        #height = int(img.shape[0] * image_scale_percent / 100)\n",
    "        #cv2.resize(img,(width,height), interpolation=cv2.INTER_CUBIC)\n",
    "        image_list.append(img)\n",
    "        csgm_list.append(the_class)\n",
    "        \n",
    "    x = np.array(image_list)\n",
    "    y = np.array(csgm_list)\n",
    "    return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape, keys=[0,1,2], filter_info={0:[32,3]}, dropout={0:0.25}, pooling={0:2}, activation='relu', loss='mean_squared_error', final_activation='sigmoid'):    \n",
    "    \n",
    "    model = models.Sequential() \n",
    "    for k in keys:\n",
    "        num_filters = filter_info[k][0]\n",
    "        filter_size = filter_info[k][1]\n",
    "        if k == keys[0]: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation = 'relu', input_shape=input_shape))\n",
    "        else: \n",
    "            model.add(Conv2D(num_filters, (filter_size,filter_size), activation= 'relu'))\n",
    "                \n",
    "        if k in pooling.keys():           \n",
    "            pool_filter_size = pooling[k]\n",
    "            model.add(MaxPooling2D(pool_size=(pool_filter_size, pool_filter_size)))            \n",
    "                 \n",
    "        if k in dropout.keys(): \n",
    "            drop_rate = dropout[k]\n",
    "            model.add(Dropout(drop_rate))\n",
    "    \n",
    "    #These will be added to the end of every model no matter what\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation=activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #Sigmoid activiation is employed in the final step ***This assumes the output is binary***. \n",
    "    model.add(Dense(1, activation=final_activation)) \n",
    "    \n",
    "    model.compile(loss=loss, \n",
    "              optimizer=optimizers.SGD(lr=1e-4),\n",
    "              metrics=['acc']) \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(exp_values, predicted_values):\n",
    "    \"\"\"\n",
    "    This creates a confusion matrix with the predicted accuracy of the model.\n",
    "    \n",
    "    exp_values must be in the format of a list and predicted values is expected to come in the format of the ouput \n",
    "    of Keras's model.predict()\n",
    "    \n",
    "    The ouput is a pandas dataframe that displays a confusion matrix indicitive of the accuracy of the model along \n",
    "        with a number score which is the accuracy of the model.\n",
    "    \"\"\"\n",
    "    predicted_values = convert_predictions(predicted_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Creates a DataFrame of zeros\n",
    "    matrix = pd.DataFrame(np.zeros((2,2)) , ['P0','P1'], ['E0','E1'])\n",
    "   \n",
    "    #Caculates whether the score was right or wrong and updates the confusion matrix \n",
    "    for i in range(len(exp_values)):\n",
    "        if exp_values[i] == predicted_values[i]:\n",
    "            matrix.iloc[[predicted_values[i]],[predicted_values[i]]] += 1\n",
    "        else:\n",
    "            matrix.iloc[[predicted_values[i]],[exp_values[i]]] += 1\n",
    "   \n",
    "    #Calculate diagonal sum and the accuracy of the model\n",
    "    #Precision (TP/TP+FPos)      Recall TP(TP+FNegative)\n",
    "    diagonal_sum = 0\n",
    "    for i in range(2):\n",
    "        diagonal_sum += matrix.iloc[i][i]\n",
    "    \n",
    "    score = diagonal_sum/len(exp_values)\n",
    "    \n",
    "  \n",
    "    return  matrix, score\n",
    "    \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(predictions): \n",
    "    \"\"\"\n",
    "    Converts predictions outputted by a keras model into a list with 1 represented the predicted output and zero \n",
    "    in other classes. \n",
    "    \"\"\"\n",
    "    l =[]\n",
    "    for p in predictions: \n",
    "        if p >= 0.5:\n",
    "            l.append(1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest(model,pooling, dropout, filter_info, file_name='model.txt', model_name='model', save_model=False, epochs=5, batch_size=32,max_accuracy=0.90):\n",
    "    \n",
    "    conn = connect()\n",
    "    try:\n",
    "        #recording_ids_dict = import_recording_ids(cache_path,conn)\n",
    "        class_data = import_class_data(conn)\n",
    "        \n",
    "        all_classes = class_data.keys()\n",
    "\n",
    "        matrices = {}\n",
    "        scores = {}\n",
    "        model_scores = {}\n",
    "    \n",
    "        #Set up to do cross-validation\n",
    "        all_recording_ids = set()\n",
    "        for a_class in all_classes:\n",
    "            for recording_id in class_data[a_class].keys():\n",
    "                all_recording_ids.add(recording_id)\n",
    "\n",
    "        #Run one fold of cross validation\n",
    "        for test_recording_id in all_recording_ids:\n",
    "            #Clean up some memory by garbage collecting variables that hold data\n",
    "            train_ids = train_ids_list = test_ids = x_train = y_train = x_test = y_test = None\n",
    "            gc.collect()\n",
    "        \n",
    "            print('Testing on {0:3} '.format(test_recording_id),end=\"\")\n",
    "            train_ids= set()\n",
    "            test_ids = set()\n",
    "        \n",
    "            #Given the recording we are using for testing, split up the raw_ids into training and testing groups\n",
    "            for recording_id in all_recording_ids:\n",
    "                raw_ids = set()\n",
    "                for a_class in all_classes:\n",
    "                    for key, values in class_data[a_class].items():\n",
    "                        if key == recording_id:\n",
    "                            for value in values:\n",
    "                                raw_ids.add(value)\n",
    "                \n",
    "                if test_recording_id == recording_id:\n",
    "                    test_ids = test_ids.union(raw_ids)\n",
    "                else: \n",
    "                    train_ids = train_ids.union(raw_ids)\n",
    "                    \n",
    "            #Create the testing set\n",
    "            x_test, y_test = create_array(conn,class_data,test_ids)\n",
    "            print()\n",
    "        \n",
    "            #Scaling the values to a value between 0 and 1\n",
    "            x_test = x_test.astype('float32')\n",
    "            x_test /= 255\n",
    "        \n",
    "            epoch = 0\n",
    "            accuracy = 0\n",
    "            no_improvement = 0\n",
    "            while epoch < epochs and accuracy < max_accuracy and no_improvement < 2:\n",
    "                #Split the train_ids up into batches that are randomly shuffled and iteratively train themodel\n",
    "                train_ids_list = list(train_ids)\n",
    "                random.shuffle(train_ids_list)\n",
    "                \n",
    "                print(\"\\tEpoch {0:5} {1:6} samples \".format(epoch,len(batch_me)),end=\"\")\n",
    "                \n",
    "                #Go through a batch at a time\n",
    "                for i in range(0,len(train_ids_list), batch_size):\n",
    "                    #Get the current batch\n",
    "                    batch = set()\n",
    "                    for j in range(i,i+batch_size):\n",
    "                        if j < len(train_ids_list):\n",
    "                            batch.add(train_ids_list[j])\n",
    "                    \n",
    "                    #Fetch the images for the batch\n",
    "                    print(\" {} \".format(i),end=\"\",flush=True)\n",
    "                    x_batch, y_batch = create_array(conn,class_data,batch)\n",
    "                \n",
    "                    #Scaling the rgb values to a value between 0 and 1\n",
    "                    x_batch = x_batch.astype('float32')\n",
    "                    x_batch /= 255\n",
    "                \n",
    "                    #Fit the model\n",
    "                    model.train_on_batch(x_batch, y_batch)\n",
    "                print(\"\",flush=True)\n",
    "              \n",
    "                #Create predictions and evaluate to find loss and accuaracy\n",
    "                predict = model.predict(x_test)\n",
    "                model_score = model.evaluate(x_test, y_test)\n",
    "                print('\\t\\tModel was ' + str(model_score[1]) + '% accurate and exhibited an average loss of ' + str(model_score[0]) + '.')\n",
    "        \n",
    "                matrix,score = confusion_matrix(y_test, predict)\n",
    "        \n",
    "                matrices.update({i : matrix})\n",
    "                print(str(matrix) + '\\n')\n",
    "                scores.update({i: score})\n",
    "                print(str(score) + '\\n')\n",
    "                model_scores.update({i:model_score})\n",
    "                epoch = epoch + 1\n",
    "                if accuracy == model_score[1]:\n",
    "                    no_improvement = no_improvement + 1\n",
    "                else:\n",
    "                    no_improvement = 0\n",
    "                accuracy = model_score[1]\n",
    "            if epoch == epochs:\n",
    "                print(\"Reached end of training run\")\n",
    "            if no_improvement >= 2:\n",
    "                print(\"Training stopped progressing\")\n",
    "            if accuracy >= max_accuracy:\n",
    "                print(\"Stopped training early because max_accuracy was reached\")\n",
    "   \n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(\"Model:\\n%s\\n\" % model.summary())\n",
    "            for key in matrices:\n",
    "                f.write(\"Baby %s\\n\" % key)\n",
    "                f.write(\"%s\\n\" % matrices[key])\n",
    "                f.write(\"%s\\n\" % scores[key])\n",
    "                f.write(\"%s\\n\" % model_scores[key])\n",
    "            \n",
    "        if save_model : \n",
    "            model.save(model)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/djp3/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/djp3/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 478, 638, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 239, 319, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 239, 319, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 237, 317, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 118, 158, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 118, 158, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 116, 156, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 58, 78, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 58, 78, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 579072)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               148242688 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 148,336,193\n",
      "Trainable params: 148,336,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keys=[0,1,2]\n",
    "filter_info={0:[32,3],1:[64,3],2:[128,3]}\n",
    "dropout={0:0.25,1:0.25,2:0.25}\n",
    "pooling={0:2,1:2,2:2}\n",
    "\n",
    "model = create_cnn( (480,640,3),\n",
    "                    keys,\n",
    "                    filter_info=filter_info,\n",
    "                    dropout=dropout,\n",
    "                    pooling=pooling,\n",
    "                    loss='mean_squared_error', \n",
    "                    final_activation='sigmoid',\n",
    "                    activation='relu')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on   2 ⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣⇣"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f77ddf3f5495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         max_accuracy=max_accuracy)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-9738100d4b05>\u001b[0m in \u001b[0;36mrunTest\u001b[0;34m(model, pooling, dropout, filter_info, file_name, model_name, save_model, epochs, batch_size, max_accuracy)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#Create the testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-998d196f9638>\u001b[0m in \u001b[0;36mcreate_array\u001b[0;34m(conn, class_data, batch_raw_ids)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mimage_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SELECT RGB_Optical_Flow from Video_Generated WHERE (raw_id=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mdb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, sql, unbuffered)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogateescape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMMAND\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOM_QUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_query_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_query_result\u001b[0;34m(self, unbuffered)\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySQLResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_status\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m             \u001b[0mfirst_packet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_packet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ok_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mbuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0mpacket_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m             \u001b[0;31m#if DEBUG: dump_packet(packet_header)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(self, num_bytes)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runTest(model=model, file_name=statistics_output_file, \n",
    "        filter_info=filter_info,  \n",
    "        dropout=dropout, \n",
    "        pooling=pooling, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        max_accuracy=max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
