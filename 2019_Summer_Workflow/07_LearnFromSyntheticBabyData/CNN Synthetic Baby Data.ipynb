{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import from the Keras library\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D,  MaxPooling2D\n",
    "from keras import optimizers \n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from secret import credentials\n",
    "\n",
    "#This allows for Keras models to be saved. \n",
    "import h5py\n",
    "#Other import statements \n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import cv2\n",
    "import pymysql\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect(): \n",
    "    db_host ='nicu-2019-03-05.c2lckhwrw1as.us-east-1.rds.amazonaws.com'\n",
    "    db_port = 3306\n",
    "    db_name = 'nicu'\n",
    "    db_username = 'jonlee'\n",
    "    db_password = 'nicu_jon'\n",
    "    \n",
    "    '''\n",
    "    db_host = credentials['db_host'];\n",
    "    db_port = credentials['db_port'];\n",
    "    db_name = credentials['db_name'];\n",
    "    db_username = credentials['db_username']\n",
    "    db_password = credentials['db_password']\n",
    "    '''\n",
    "    \n",
    "    conn = pymysql.connect(db_host, user=db_username, port=db_port, passwd=db_password, db=db_name)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_data(cache_path, conn=connect()): \n",
    "\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    recording_ids = {}\n",
    "    xy = {}    \n",
    "        \n",
    "    image_query = \"SELECT r.id, r.recording_id, r.isCSGM FROM nicu.Video_Raw AS r JOIN nicu.Video_Generated AS g ON r.id=g.raw_id  WHERE (r.recording_id>1) AND (g.RGB_Optical_Flow IS NOT NULL) LIMIT 2000\"\n",
    "    try:\n",
    "        curs.execute(image_query) #(list(recording_ids.keys())))\n",
    "        for row in curs.fetchall():\n",
    "            raw_id = row[0]\n",
    "            rec_id = row[1]\n",
    "            csgm = row[2]\n",
    "            if rec_id in recording_ids:\n",
    "                recording_ids.get(rec_id).append(raw_id)\n",
    "            else:\n",
    "                recording_ids.update({rec_id:[raw_id]})\n",
    "            xy.update({raw_id:[csgm]})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving ID's\", e)\n",
    "        raise e\n",
    "        \n",
    "    for rec_id in recording_ids:\n",
    "        #cache_path = cache_path+\"recording_\"+(\"{:02d}\".format(rec_id))\n",
    "        cache_path = cache_path+'testing'\n",
    "        if not os.path.exists(cache_path):\n",
    "            os.mkdir(cache_path)\n",
    "        raw_id_list = recording_ids.get(rec_id)\n",
    "        for raw_id in raw_id_list:\n",
    "            current_input = cache_path+'/'+str(raw_id)+\".oflow.png\"\n",
    "            if not os.path.exists(current_input):\n",
    "                try:\n",
    "                    image_query = \"SELECT RGB_Optical_Flow from Video_Generated WHERE (raw_id=%s)\"\n",
    "                    curs.execute(image_query, (str(raw_id)))\n",
    "                    for row in curs.fetchall():\n",
    "                        db_img = row[0]\n",
    "                        if db_img is not None:\n",
    "                            img=cv2.imdecode(np.asarray(bytearray(db_img),dtype=np.uint8),cv2.IMREAD_UNCHANGED)\n",
    "                            cv2.imwrite(current_input,img)\n",
    "                except Exception as e:\n",
    "                        print(\"Error retrieving Optical Flow frame\",e)\n",
    "                        raise e\n",
    "            #Resizing the image to a quarter of the size\n",
    "            img = cv2.imread(current_input)\n",
    "            scale_percent = 20\n",
    "            width = int(img.shape[1] * scale_percent / 100)\n",
    "            height = int(img.shape[0] * scale_percent / 100)\n",
    "            cv2.resize(img,(width,height), interpolation=cv2.INTER_CUBIC)\n",
    "            xy.get(raw_id).insert(0,img)\n",
    "    curs.close()\n",
    "    \n",
    "    return recording_ids, xy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_array(raw_ids, xy):    \n",
    "    image_list=[]\n",
    "    csgm_list=[]\n",
    "    \n",
    "    random.shuffle(raw_ids)\n",
    "    \n",
    "    for i in raw_ids:\n",
    "        #if not xy.get(i)[0] == None:\n",
    "        image_list.append(xy.get(i)[0])\n",
    "        csgm_list.append(xy.get(i)[1])\n",
    "    x = np.array(image_list)\n",
    "    y = np.array(csgm_list)\n",
    "    return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x_train):    \n",
    "    model = models.Sequential() \n",
    "\n",
    "    #The model will learn 32 filters in this layer\n",
    "    model.add(Conv2D(32, (3,3), activation = 'relu', input_shape=x_train.shape[1:])) # this applies 32 convolution filters of size 3x3 each\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "\n",
    "    #Max Pooling takes a 2x2 grid and takes the highest value of the grid and resizes based on that max value\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Sigmoid activiation is emplpyed in the final step because the output is binary. \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    \n",
    "    model.compile(loss='mean_squared_error', \n",
    "              optimizer=optimizers.SGD(lr=1e-4),\n",
    "              metrics=['acc']) \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(exp_values, predicted_values):\n",
    "    \"\"\"\n",
    "    This creates a confusion matrix with the predicted accuracy of the model.\n",
    "    \n",
    "    exp_values must be in the format of a list and predicted values is expected to come in the format of the ouput \n",
    "    of Keras's model.predict()\n",
    "    \n",
    "    The ouput is a pandas dataframe that displays a confusion matrix indicitive of the accuracy of the model along \n",
    "        with a number score which is the accuracy of the model.\n",
    "    \"\"\"\n",
    "    predicted_values = convert_predictions(predicted_values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Creates a DataFrame of zeros\n",
    "    matrix = pd.DataFrame(np.zeros((2,2)) , ['P0','P1'], ['E0','E1'])\n",
    "   \n",
    "    #Caculates whether the score was right or wrong and updates the confusion matrix \n",
    "    for i in range(len(exp_values)):\n",
    "        if exp_values[i] == predicted_values[i]:\n",
    "            matrix.iloc[[predicted_values[i]],[predicted_values[i]]] += 1\n",
    "        else:\n",
    "            matrix.iloc[[predicted_values[i]],[exp_values[i]]] += 1\n",
    "   \n",
    "    #Calculate diagonal sum and the accuracy of the model\n",
    "    diagonal_sum = 0\n",
    "    for i in range(2):\n",
    "        diagonal_sum += matrix.iloc[i][i]\n",
    "    \n",
    "    score = diagonal_sum/len(exp_values)\n",
    "    \n",
    "  \n",
    "    return  matrix, score\n",
    "    \n",
    "    \n",
    "            \n",
    "def convert_predictions(predictions): \n",
    "    \"\"\"\n",
    "    Converts predictions outputted by a keras model into a list with 1 represented the predicted output and zero \n",
    "    in other classes. \n",
    "    \"\"\"\n",
    "    l =[]\n",
    "    for p in predictions: \n",
    "        if p >= 0.5:\n",
    "            l.append(1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runTest(file_name, model_name, save_model=False, epochs=5, batch_size=32):\n",
    "    matrices = {}\n",
    "    scores = {}\n",
    "    model_scores = {}\n",
    "    \n",
    "    #Dr. Patterson - you will need to update this line of code for it to work in your directory\n",
    "    recording_ids_dict, xy = import_data('/Users/jonathanlee/Desktop/Python/NICU/NICU_data')\n",
    "    \n",
    "\n",
    "    for i in recording_ids_dict:\n",
    "        print('Testing on ' + str(i))\n",
    "        train_ids= []\n",
    "        test_ids = []\n",
    "        \n",
    "        for j in recording_ids_dict:\n",
    "            if j == i:\n",
    "                test_ids = recording_ids_dict[j]\n",
    "            else: \n",
    "                train_ids.extend(list(recording_ids_dict[j]))\n",
    "        \n",
    "        x_train, y_train = create_array(train_ids, xy)\n",
    "        x_test, y_test = create_array(test_ids, xy)\n",
    "        \n",
    "        #Scaling the values to a value between 0 and 1\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        \n",
    "        \n",
    "        model = create_model(x_train)\n",
    "        \n",
    "        #Fit the model\n",
    "        model.fit(x_train, y_train, epochs = epochs)\n",
    "        \n",
    "        #Create predictions and evaluate to find loss and accuaracy\n",
    "        predict = model.predict(x_test)\n",
    "        model_score = model.evaluate(x_test, y_test)\n",
    "        print('Model was ' + str(model_score[1]) + '% accurate and exhibited an average loss of ' + str(model_score[0]) + '.')\n",
    "        \n",
    "        matrix,score = confusion_matrix(y_test, predict)\n",
    "        \n",
    "        matrices.update({i : matrix})\n",
    "        print(str(matrix) + '\\n')\n",
    "        scores.update({i: score})\n",
    "        print(str(score) + '\\n')\n",
    "        model_scores.update({i:model_score})\n",
    "   \n",
    "    with open(file_name, 'w') as f:\n",
    "        for key in matrices:\n",
    "            f.write(\"Baby %s\\n\" % key)\n",
    "            f.write(\"%s\\n\" % matrices[key])\n",
    "            f.write(\"%s\\n\" % scores[key])\n",
    "            f.write(\"%s\\n\" % model_scores[key])\n",
    "            \n",
    "    if save_model : \n",
    "        model.save(model)\n",
    "    #Add a final matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 2\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 96s 8s/step - loss: 0.2483 - acc: 0.8333\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 155s 13s/step - loss: 0.2472 - acc: 0.8333\n",
      "10/10 [==============================] - 14s 1s/step\n",
      "Model was 0.30000001192092896% accurate and exhibited an average loss of 0.2505987286567688.\n",
      "     E0   E1\n",
      "P0  3.0  0.0\n",
      "P1  7.0  0.0\n",
      "\n",
      "0.3\n",
      "\n",
      "Testing on 3\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 140s 12s/step - loss: 0.2470 - acc: 0.9167\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 105s 9s/step - loss: 0.2479 - acc: 0.9167\n",
      "10/10 [==============================] - 10s 1s/step\n",
      "Model was 0.800000011920929% accurate and exhibited an average loss of 0.24938659369945526.\n",
      "     E0   E1\n",
      "P0  8.0  0.0\n",
      "P1  2.0  0.0\n",
      "\n",
      "0.8\n",
      "\n",
      "Testing on 4\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 154s 8s/step - loss: 0.2484 - acc: 0.6500\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 422s 21s/step - loss: 0.2498 - acc: 0.6000\n",
      "2/2 [==============================] - 3s 1s/step\n",
      "Model was 1.0% accurate and exhibited an average loss of 0.24967898428440094.\n",
      "     E0   E1\n",
      "P0  2.0  0.0\n",
      "P1  0.0  0.0\n",
      "\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [\"LeftArm\", \"RightArm\", \"LeftLeg\",\"RightLeg\"]\n",
    "names = [\"/Isaiah\", \"/Kaylee\", \"/Patterson\", \"/Ryan\"]\n",
    "\n",
    "#File name for the statistics to be save in. Must include .txt at the end\n",
    "file_name = 'test.txt'\n",
    "\n",
    "#Must have the h5py package installed or the model will not save. This should be the path of the location you would like\n",
    "#To save the model\n",
    "model_file_name = 'test'\n",
    "\n",
    "#Feel free to change the amount of epochs being run\n",
    "runTest(file_name, model_file_name, epochs=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
